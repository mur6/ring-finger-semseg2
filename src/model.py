import math
from typing import Optional, Tuple, Union

import albumentations as A
import numpy as np
import torch
from torch import nn
from torch.nn import CrossEntropyLoss
from torch.utils.data import Dataset
from torchvision import transforms
from torchvision import transforms as transforms
from transformers import SegformerDecodeHead, SegformerForSemanticSegmentation
from transformers.modeling_outputs import SemanticSegmenterOutput
from transformers.models.segformer.modeling_segformer import SegformerMLP, SegformerModel


class OrigSegformerDecodeHead(SegformerDecodeHead):
    def __init__(self, config):
        super().__init__(config)
        # linear layers which will unify the channel dimension of each of the encoder blocks to the same config.decoder_hidden_size
        mlps = []
        for i in range(config.num_encoder_blocks):
            mlp = SegformerMLP(config, input_dim=config.hidden_sizes[i])
            mlps.append(mlp)
        self.linear_c = nn.ModuleList(mlps)

        # the following 3 layers implement the ConvModule of the original implementation
        self.linear_fuse = nn.Conv2d(
            in_channels=config.decoder_hidden_size * config.num_encoder_blocks,
            out_channels=config.decoder_hidden_size,
            kernel_size=1,
            bias=False,
        )
        self.batch_norm = nn.BatchNorm2d(config.decoder_hidden_size)
        self.activation = nn.ReLU()

        self.dropout = nn.Dropout(config.classifier_dropout_prob)
        self.classifier = nn.Conv2d(config.decoder_hidden_size, config.num_labels, kernel_size=1)
        print(f"decoder_hidden_size={config.decoder_hidden_size} num_labels={config.num_labels}")
        # self.dropout2 = nn.Dropout()
        # self.fc = nn.Sequential(
        #     nn.Dropout(0.5), nn.Linear(768 * 128 * 128, 512), nn.ReLU(True), nn.Dropout(0.5), nn.Linear(512, 4)
        # )
        # self.classifier2 = nn.Linear(768 * 128 * 128, 4)
        print(config.decoder_hidden_size)
        self.classifier2 = nn.Conv2d(config.decoder_hidden_size, 16, kernel_size=1)
        self.fc = nn.Sequential(
            nn.Dropout(),
            nn.Linear(16 * 128 * 128, 512),
            nn.ReLU(True),
            nn.Dropout(0.5),
            nn.Linear(512, 4),
        )
        self.config = config

    def forward(self, encoder_hidden_states):
        batch_size = encoder_hidden_states[-1].shape[0]

        all_hidden_states = ()
        for encoder_hidden_state, mlp in zip(encoder_hidden_states, self.linear_c):
            if self.config.reshape_last_stage is False and encoder_hidden_state.ndim == 3:
                height = width = int(math.sqrt(encoder_hidden_state.shape[-1]))
                encoder_hidden_state = (
                    encoder_hidden_state.reshape(batch_size, height, width, -1).permute(0, 3, 1, 2).contiguous()
                )

            # unify channel dimension
            height, width = encoder_hidden_state.shape[2], encoder_hidden_state.shape[3]
            encoder_hidden_state = mlp(encoder_hidden_state)
            encoder_hidden_state = encoder_hidden_state.permute(0, 2, 1)
            encoder_hidden_state = encoder_hidden_state.reshape(batch_size, -1, height, width)
            # upsample
            encoder_hidden_state = nn.functional.interpolate(
                encoder_hidden_state, size=encoder_hidden_states[0].size()[2:], mode="bilinear", align_corners=False
            )
            all_hidden_states += (encoder_hidden_state,)

        hidden_states = self.linear_fuse(torch.cat(all_hidden_states[::-1], dim=1))
        hidden_states = self.batch_norm(hidden_states)
        hidden_states = self.activation(hidden_states)
        hidden_states = self.dropout(hidden_states)
        # print("after dropout: nn.Dropout: ", hidden_states.shape)
        # logits are of shape (batch_size, num_labels, height/4, width/4)
        logits = self.classifier(hidden_states)
        # print("after classifier: nn.Linear: ", logits.shape)
        # temp = self.dropout2(temp)
        # points = self.classifier2(temp)
        # print("1", hidden_states.shape)
        temp = self.classifier2(hidden_states)
        # print("2", temp.shape)
        temp = temp.contiguous().view(batch_size, -1)
        # print("3", temp.shape)
        points = self.fc(temp)
        return logits, points


class OrigSegformerForSemanticSegmentation(SegformerForSemanticSegmentation):
    def __init__(self, config):
        super().__init__(config)
        self.segformer = SegformerModel(config)
        self.decode_head = OrigSegformerDecodeHead(config)

        # Initialize weights and apply final processing
        self.post_init()

    def forward(
        self,
        pixel_values: torch.FloatTensor,
        labels: Optional[torch.LongTensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, SemanticSegmenterOutput]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):
            Ground truth semantic segmentation maps for computing the loss. Indices should be in `[0, ...,
            config.num_labels - 1]`. If `config.num_labels > 1`, a classification loss is computed (Cross-Entropy).

        Returns:

        Examples:

        ```python
        >>> from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation
        >>> from PIL import Image
        >>> import requests

        >>> feature_extractor = SegformerFeatureExtractor.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512")
        >>> model = SegformerForSemanticSegmentation.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512")

        >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
        >>> image = Image.open(requests.get(url, stream=True).raw)

        >>> inputs = feature_extractor(images=image, return_tensors="pt")
        >>> outputs = model(**inputs)
        >>> logits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)
        >>> list(logits.shape)
        [1, 150, 128, 128]
        ```"""
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )

        outputs = self.segformer(
            pixel_values,
            output_attentions=output_attentions,
            output_hidden_states=True,  # we need the intermediate hidden states
            return_dict=return_dict,
        )

        encoder_hidden_states = outputs.hidden_states if return_dict else outputs[1]

        logits, points = self.decode_head(encoder_hidden_states)

        loss = None
        if labels is not None:
            if not self.config.num_labels > 1:
                raise ValueError("The number of labels should be greater than one")
            else:
                # upsample logits to the images' original size
                upsampled_logits = nn.functional.interpolate(
                    logits, size=labels.shape[-2:], mode="bilinear", align_corners=False
                )
                loss_fct = CrossEntropyLoss(ignore_index=self.config.semantic_loss_ignore_index)
                loss = loss_fct(upsampled_logits, labels)

        if not return_dict:
            if output_hidden_states:
                output = (logits,) + outputs[1:]
            else:
                output = (logits,) + outputs[2:]
            return ((loss,) + output) if loss is not None else output

        # return SemanticSegmenterOutput(
        #     loss=loss,
        #     logits=logits,
        #     hidden_states=outputs.hidden_states if output_hidden_states else None,
        #     attentions=outputs.attentions,
        # )
        return logits, points


def get_model(model_dir):
    id2label = {0: "unlabeled", 1: "hand", 2: "mat"}
    label2id = {v: k for k, v in id2label.items()}
    model_dir = "models/segformer_b2/"
    model = OrigSegformerForSemanticSegmentation.from_pretrained(model_dir)
    # model = OrigSegformerForSemanticSegmentation.from_pretrained(
    #     "nvidia/mit-b2",
    #     ignore_mismatched_sizes=True,
    #     num_labels=len(id2label),
    #     id2label=id2label,
    #     label2id=label2id,
    #     reshape_last_stage=True,
    # )
    return model


if __name__ == "__main__":
    model_dir = "models/segformer_b2/"
    model = get_model(model_dir)
    logits, points = model(torch.rand(1, 3, 512, 512))
    print(logits.shape, points.shape)
    # print(model)
